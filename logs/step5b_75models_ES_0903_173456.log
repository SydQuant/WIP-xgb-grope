2025-09-03 17:34:57,810 - __main__ - INFO - \U0001f4c4 Loaded configuration from configs/individual_target_test.yaml
2025-09-03 17:34:57,810 - __main__ - INFO - \U0001f4c4 Using configuration: configs/individual_target_test.yaml
2025-09-03 17:34:57,810 - __main__ - INFO - \U0001f680 Starting XGB Ensemble Analysis
2025-09-03 17:34:57,810 - __main__ - INFO - \U0001f3af Target symbols: ['@ES#C']
2025-09-03 17:34:57,810 - __main__ - INFO - 
================================================================================
2025-09-03 17:34:57,810 - __main__ - INFO - \U0001f504 PROCESSING TARGET 1/1: @ES#C
2025-09-03 17:34:57,810 - __main__ - INFO - ================================================================================
2025-09-03 17:34:57,810 - __main__ - INFO - \U0001f504 Loading real market data for target: @ES#C
2025-09-03 17:34:57,810 - data.data_utils_simple - INFO - Preparing data for @ES#C, period 2020-07-01 to 2024-08-01
2025-09-03 17:34:57,956 - data.data_utils_simple - INFO - Loaded data for 10 symbols
C:\Users\zhang\Desktop\Steve\bond_ls_xgb_grope_full_v6\data\data_utils_simple.py:279: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[target_col] = target_returns.reindex(df.index)
2025-09-03 17:34:58,636 - data.data_utils_simple - INFO - Cleaning data: (1057, 523)
2025-09-03 17:34:58,779 - data.data_utils_simple - INFO - Cleaned data: (1057, 523) (dropped 0 features)
2025-09-03 17:34:58,782 - data.data_utils_simple - INFO - Final dataset: (1057, 523), target mean=0.000496, target std=0.009404
2025-09-03 17:34:58,789 - __main__ - INFO - \u2705 Loaded data: X shape (1057, 522), y shape (1057,)
2025-09-03 17:34:58,789 - __main__ - INFO - \U0001f4c5 Date range: 2020-07-01 12:00:00 to 2024-07-31 12:00:00
2025-09-03 17:34:58,789 - __main__ - INFO - Loaded real data: X shape (1057, 522), y shape (1057,)
2025-09-03 17:34:58,789 - __main__ - INFO - Date range: 2020-07-01 12:00:00 to 2024-07-31 12:00:00
2025-09-03 17:34:58,789 - __main__ - INFO - Using CLI specified feature count: 50
2025-09-03 17:34:58,789 - __main__ - INFO - Applying smart block-wise feature selection: 522 -> 50 features
2025-09-03 17:34:58,789 - model.feature_selection - INFO - Smart block-wise selection: 522 -> blocks of 100 -> local clustering -> global deduplication
2025-09-03 17:34:58,789 - model.feature_selection - INFO - Processing block 1/6: 100 features
2025-09-03 17:34:59,245 - model.feature_selection - INFO - Block 100: Selected 15 features (best: 0.0823)
2025-09-03 17:34:59,245 - model.feature_selection - INFO - Processing block 2/6: 100 features
2025-09-03 17:34:59,716 - model.feature_selection - INFO - Block 100: Selected 15 features (best: 0.0681)
2025-09-03 17:34:59,716 - model.feature_selection - INFO - Processing block 3/6: 100 features
2025-09-03 17:35:00,195 - model.feature_selection - INFO - Block 100: Selected 15 features (best: 0.0682)
2025-09-03 17:35:00,195 - model.feature_selection - INFO - Processing block 4/6: 100 features
2025-09-03 17:35:00,676 - model.feature_selection - INFO - Block 100: Selected 15 features (best: 0.1273)
2025-09-03 17:35:00,676 - model.feature_selection - INFO - Processing block 5/6: 100 features
2025-09-03 17:35:01,137 - model.feature_selection - INFO - Block 100: Selected 15 features (best: 0.0789)
2025-09-03 17:35:01,137 - model.feature_selection - INFO - Processing block 6/6: 22 features
2025-09-03 17:35:01,246 - model.feature_selection - INFO - Block 22: Selected 8 features (best: 0.0739)
2025-09-03 17:35:01,246 - model.feature_selection - INFO - Global deduplication: 83 candidates -> removing cross-block correlations
2025-09-03 17:35:01,373 - model.feature_selection - INFO - \u2705 Smart block-wise selection complete: 50 features selected
2025-09-03 17:35:01,373 - model.feature_selection - INFO -    Local clustering: 0.7, Global clustering: 0.7
2025-09-03 17:35:01,374 - __main__ - INFO - Feature selection complete: (1057, 50)
2025-09-03 17:35:01,375 - __main__ - INFO - Using 6-fold walk-forward cross-validation
2025-09-03 17:35:01,375 - __main__ - INFO - Using standard XGBoost architecture with 75 models
2025-09-03 17:35:01,648 - __main__ - INFO - [Fold 0] Training data: X_tr(252, 50), y_tr(252,)
2025-09-03 17:35:01,648 - __main__ - INFO - [Fold 0] Target stats: mean=0.001273, std=0.008950
2025-09-03 17:35:01,650 - __main__ - INFO - [Fold 0] Feature stats: 393.456325 total magnitude
C:\Users\zhang\anaconda3\Lib\site-packages\xgboost\core.py:729: UserWarning: [17:35:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\common\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

  return func(**kwargs)
2025-09-03 17:36:28,038 - __main__ - INFO - Fold 0 XGBoost predictions: train_preds count=75, test_preds count=75
2025-09-03 17:36:28,038 - __main__ - INFO - Fold 0 test prediction stats: model_0: mean=0.001319, std=0.009138, range=[-0.025416, 0.026818]; model_1: mean=0.001281, std=0.009686, range=[-0.029023, 0.028040]; model_2: mean=0.001389, std=0.008503, range=[-0.023015, 0.025610]
2025-09-03 17:36:29,750 - __main__ - INFO - Fold 0: Selected 12 models, weights: ['0.406', '0.011', '0.024', '0.023', '0.089', '0.039', '0.364', '0.016', '0.004', '0.002', '0.016', '0.006'], tau: 0.551
2025-09-03 17:36:29,750 - __main__ - INFO - Fold 0 individual test signals: count=12, lengths=[176, 176, 176], magnitudes=[79.1634197781669, 79.82762030770189, 78.93050381640671]
2025-09-03 17:36:29,752 - __main__ - INFO - Fold 0 signal stats: mean=-0.010046, std=0.559128, sum=-1.768036, magnitude=79.105118
2025-09-03 17:36:29,753 - __main__ - INFO - [Fold 1] Training data: X_tr(252, 50), y_tr(252,)
2025-09-03 17:36:29,753 - __main__ - INFO - [Fold 1] Target stats: mean=0.001273, std=0.008950
2025-09-03 17:36:29,756 - __main__ - INFO - [Fold 1] Feature stats: 393.456325 total magnitude
2025-09-03 17:38:07,636 - __main__ - INFO - Fold 1 XGBoost predictions: train_preds count=75, test_preds count=75
2025-09-03 17:38:07,637 - __main__ - INFO - Fold 1 test prediction stats: model_0: mean=0.001049, std=0.004738, range=[-0.020083, 0.013980]; model_1: mean=0.001530, std=0.005248, range=[-0.020652, 0.016252]; model_2: mean=0.000851, std=0.004607, range=[-0.019427, 0.012861]
2025-09-03 17:38:09,285 - __main__ - INFO - Fold 1: Selected 12 models, weights: ['0.290', '0.000', '0.000', '0.000', '0.690', '0.000', '0.017', '0.001', '0.002', '0.000', '0.000', '0.000'], tau: 0.248
2025-09-03 17:38:09,285 - __main__ - INFO - Fold 1 individual test signals: count=12, lengths=[176, 176, 176], magnitudes=[72.36618706762246, 76.80118245406318, 70.13101589717729]
2025-09-03 17:38:09,287 - __main__ - INFO - Fold 1 signal stats: mean=0.021477, std=0.493898, sum=3.779962, magnitude=71.401831
2025-09-03 17:38:09,287 - __main__ - INFO - [Fold 2] Training data: X_tr(352, 50), y_tr(352,)
2025-09-03 17:38:09,288 - __main__ - INFO - [Fold 2] Target stats: mean=0.001167, std=0.008346
2025-09-03 17:38:09,289 - __main__ - INFO - [Fold 2] Feature stats: 397.803423 total magnitude
2025-09-03 17:39:50,371 - __main__ - INFO - Fold 2 XGBoost predictions: train_preds count=75, test_preds count=75
2025-09-03 17:39:50,371 - __main__ - INFO - Fold 2 test prediction stats: model_0: mean=0.001230, std=0.004467, range=[-0.009709, 0.012868]; model_1: mean=0.002586, std=0.005723, range=[-0.011854, 0.017120]; model_2: mean=0.002253, std=0.004021, range=[-0.008398, 0.013245]
2025-09-03 17:39:52,037 - __main__ - INFO - Fold 2: Selected 12 models, weights: ['0.000', '0.000', '0.000', '0.001', '0.022', '0.000', '0.062', '0.002', '0.013', '0.000', '0.214', '0.687'], tau: 0.200
2025-09-03 17:39:52,038 - __main__ - INFO - Fold 2 individual test signals: count=12, lengths=[176, 176, 176], magnitudes=[97.42608168773346, 88.4613632715129, 94.9167775541503]
2025-09-03 17:39:52,039 - __main__ - INFO - Fold 2 signal stats: mean=0.071198, std=0.542575, sum=12.530921, magnitude=82.242401
2025-09-03 17:39:52,039 - __main__ - INFO - [Fold 3] Training data: X_tr(528, 50), y_tr(528,)
2025-09-03 17:39:52,040 - __main__ - INFO - [Fold 3] Target stats: mean=0.000483, std=0.010169
2025-09-03 17:39:52,042 - __main__ - INFO - [Fold 3] Feature stats: 393.339343 total magnitude
2025-09-03 17:41:41,207 - __main__ - INFO - Fold 3 XGBoost predictions: train_preds count=75, test_preds count=75
2025-09-03 17:41:41,208 - __main__ - INFO - Fold 3 test prediction stats: model_0: mean=-0.001332, std=0.005302, range=[-0.015688, 0.011968]; model_1: mean=-0.001599, std=0.005871, range=[-0.018134, 0.015541]; model_2: mean=-0.001686, std=0.005330, range=[-0.018119, 0.011469]
2025-09-03 17:41:42,918 - __main__ - INFO - Fold 3: Selected 12 models, weights: ['0.491', '0.000', '0.274', '0.006', '0.000', '0.000', '0.001', '0.000', '0.015', '0.213', '0.000', '0.000'], tau: 0.377
2025-09-03 17:41:42,918 - __main__ - INFO - Fold 3 individual test signals: count=12, lengths=[176, 176, 176], magnitudes=[86.90086707060044, 87.59557502908933, 91.9905728849622]
2025-09-03 17:41:42,919 - __main__ - INFO - Fold 3 signal stats: mean=0.007694, std=0.467246, sum=1.354147, magnitude=67.197454
2025-09-03 17:41:42,920 - __main__ - INFO - [Fold 4] Training data: X_tr(704, 50), y_tr(704,)
2025-09-03 17:41:42,920 - __main__ - INFO - [Fold 4] Target stats: mean=0.000319, std=0.010462
2025-09-03 17:41:42,921 - __main__ - INFO - [Fold 4] Feature stats: 392.040051 total magnitude
2025-09-03 17:43:38,505 - __main__ - INFO - Fold 4 XGBoost predictions: train_preds count=75, test_preds count=75
2025-09-03 17:43:38,505 - __main__ - INFO - Fold 4 test prediction stats: model_0: mean=-0.000824, std=0.005513, range=[-0.019346, 0.013044]; model_1: mean=0.000284, std=0.005717, range=[-0.023250, 0.012847]; model_2: mean=-0.000036, std=0.004473, range=[-0.016649, 0.013045]
2025-09-03 17:43:40,308 - __main__ - INFO - Fold 4: Selected 12 models, weights: ['0.731', '0.204', '0.001', '0.038', '0.003', '0.001', '0.001', '0.001', '0.001', '0.016', '0.002', '0.001'], tau: 0.592
2025-09-03 17:43:40,308 - __main__ - INFO - Fold 4 individual test signals: count=12, lengths=[176, 176, 176], magnitudes=[91.2256501074218, 87.68559135235304, 93.27351656460979]
2025-09-03 17:43:40,309 - __main__ - INFO - Fold 4 signal stats: mean=-0.064486, std=0.497884, sum=-11.349475, magnitude=73.018489
2025-09-03 17:43:40,310 - __main__ - INFO - [Fold 5] Training data: X_tr(880, 50), y_tr(880,)
2025-09-03 17:43:40,310 - __main__ - INFO - [Fold 5] Target stats: mean=0.000397, std=0.009850
2025-09-03 17:43:40,311 - __main__ - INFO - [Fold 5] Feature stats: 390.687900 total magnitude
2025-09-03 17:45:27,655 - __main__ - INFO - Fold 5 XGBoost predictions: train_preds count=75, test_preds count=75
2025-09-03 17:45:27,656 - __main__ - INFO - Fold 5 test prediction stats: model_0: mean=0.000297, std=0.004468, range=[-0.013278, 0.010434]; model_1: mean=0.000668, std=0.004448, range=[-0.016495, 0.013336]; model_2: mean=0.000249, std=0.004152, range=[-0.010015, 0.010987]
2025-09-03 17:45:29,821 - __main__ - INFO - Fold 5: Selected 12 models, weights: ['0.000', '0.000', '0.000', '0.046', '0.000', '0.000', '0.000', '0.477', '0.000', '0.477', '0.000', '0.000'], tau: 0.200
2025-09-03 17:45:29,821 - __main__ - INFO - Fold 5 individual test signals: count=12, lengths=[177, 177, 177], magnitudes=[94.62022820731515, 92.78901860858063, 97.08622620438987]
2025-09-03 17:45:29,823 - __main__ - INFO - Fold 5 signal stats: mean=0.019917, std=0.470729, sum=3.525331, magnitude=67.056953
2025-09-03 17:45:29,823 - __main__ - INFO - \U0001f4cb Processing complete: 6 fold summaries created
2025-09-03 17:45:29,823 - __main__ - INFO - \U0001f4ca OOS signal magnitude: 440.0222465325
2025-09-03 17:45:29,823 - __main__ - INFO - OOS DAPY(hits): 44.58 | OOS IR: 0.91 | OOS hit-rate: 0.588
2025-09-03 17:45:29,845 - __main__ - INFO - Calculating comprehensive performance metrics...
2025-09-03 17:45:29,847 - metrics.performance_report - INFO - Performance summary saved to artifacts/performance_summary.csv
2025-09-03 17:45:29,853 - diagnostics.diagnostic_output - INFO - \U0001f4be Saved comprehensive diagnostics to artifacts/diagnostics/diagnostic_ESC_20250903_174529.json
2025-09-03 17:45:29,853 - diagnostics.diagnostic_output - INFO - \U0001f4c4 Saved diagnostic summary to artifacts/diagnostics/summary_ESC_20250903_174529.txt
2025-09-03 17:45:29,856 - __main__ - INFO - \u2705 Completed @ES#C
2025-09-03 17:45:29,856 - __main__ - INFO - 
================================================================================
2025-09-03 17:45:29,856 - __main__ - INFO - \U0001f4ca FINAL SUMMARY: 1/1 targets completed successfully
2025-09-03 17:45:29,856 - __main__ - INFO - ================================================================================
GPU acceleration enabled for XGBoost
OOS Shuffling p-value (DAPY): 0.0049751244 (obs=44.58)

==================================================
            OUT-OF-SAMPLE PERFORMANCE             
==================================================

RETURNS:
  Total Return:          30.33%
  Annualized Return:      7.23%
  Volatility:             7.92%
  Sharpe Ratio:            0.91

RISK:
  Max Drawdown:         -13.30%

ACCURACY:
  Win Rate:              48.63%
  Hit Rate:              48.63%
  Avg Win:               0.0034
  Avg Loss:             -0.0030

TRADES:
  Total Trades:            1057
  Winning Trades:           514
  Losing Trades:            485
==================================================
